# -*- coding: utf-8 -*-
"""AT_Project6_PulsarStarPrediction_shravani

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13aghPgX8TNlrGlIUJ8vAA0ijHqepQeHQ

# Project 6: Predicting A Pulsar Star

### Overview 

Pulsar stars (https://www.youtube.com/watch?v=gjLk_72V9Bw) are a rare type of Neutron stars (https://www.youtube.com/watch?v=hCwDNXKlN8Q) that produce radio emissions detectable on Earth. They are of considerable scientific interest as probes of space-time, the interstellar medium, and states of matter.

You can learn more about the Pulsar and neutron star by watching this video: https://www.youtube.com/watch?v=RrMvUL8HFlM

As pulsars rotate, their emission beam sweeps across the sky, and when this crosses our line of sight, it produces a detectable pattern of broadband radio emission. 

<img src='https://student-datasets-bucket.s3.ap-south-1.amazonaws.com/images/lightcurve_pulsar.gif'>

As pulsars rotate rapidly, this pattern repeats periodically. Thus pulsar search involves looking for periodic radio signals using large radio telescopes.

Each pulsar produces a slightly different emission pattern, which varies slightly with each rotation. Thus a potential signal detection known as a 'candidate', is averaged over many rotations of the pulsar, as determined by the length of observation. In the absence of additional info, each candidate could potentially describe a real pulsar. However, in practice almost all detections are caused by Radio Frequency Interference (RFI) and noise, making legitimate signals hard to find.

Machine learning tools are now being used to automatically label pulsar candidates to facilitate rapid analysis. The classification algorithms, in particular, are being widely adopted, which treat the candidate datasets as binary classification problems (predict either `0` or `1`). Here, the legitimate pulsar examples are a minority positive class (less in numbers), and the remaining examples are a majority negative class.

The class labels used are `0` (negative class) and `1` (positive class). Hence, **we need to deploy the XGBoost Classifier classification model which can accurately detect the class 1 examples.**

---

### Attribute Information

Each candidate is described by 8 continuous variables and a single class variable. The first four are simple statistics obtained from the integrated pulse profile (folded profile). This is an array of continuous variables that describe a longitude-resolved version of the signal that has been averaged in both time and frequency. The remaining four variables are similarly obtained from the DM-SNR curve. **You do not have to worry about what they actually mean.** These 8 variables are summarised below:

1. The mean of the integrated profile.

2. The standard deviation of the integrated profile.

3. Excess kurtosis of the integrated profile.

4. The skewness of the integrated profile.

5. The mean of the DM-SNR curve.

6. The standard deviation of the DM-SNR curve.

7. Excess kurtosis of the DM-SNR curve.

8. The skewness of the DM-SNR curve.

Source: https://archive.ics.uci.edu/ml/datasets/HTRU2

**Courtesy**

Dr Robert Lyon

University of Manchester

School of Physics and Astronomy

Alan Turing Building

Manchester M13 9PL

United Kingdom

robert.lyon@manchester.ac.uk

#### Dataset Links

You can get the datasets from the following links:

1. Train dataset: https://student-datasets-bucket.s3.ap-south-1.amazonaws.com/whitehat-ds-datasets/project-5/pulsar-star-prediction-train.csv

2. Test dataset: https://student-datasets-bucket.s3.ap-south-1.amazonaws.com/whitehat-ds-datasets/project-5/pulsar-star-prediction-test.csv

---

### Project Requirements

1. Create a pandas DataFrame for both the train and test datasets.

2. Display the first five rows of both the training and test DataFrames.

3. Display the last five rows of both the training and test DataFrames.

4. Find the number of rows and columns in both the train and test DataFrames.

5. Check for the missing values in both the train and test DataFrames.

6. Count the number of `0` and `1` classes in the training dataset.

7. Separate the feature variables, i.e, `x_train` and `x_test` from both the DataFrames.

8. Separate the target variable, i.e, `y_train` and `y_test` from both the DataFrames.

9. Apply the `XGBClassifier` machine learning model to predict the `0` and `1` classes in the test dataset, i.e, `x_test`.

10. Print the confusion matrix and the classification report to evaluate your prediction model. Also, based on the confusion matrix, precision, recall and f1-score values, report whether the prediction model deployed by you is making accurate predictions or not.

---

#### 1. Create The DataFrames

Create the Pandas DataFrames for both the training and test datasets.
"""

# Create the DataFrames for both the train and test datasets and store them in the 'train_df' and 'test_df' variables respectively.
import pandas as pd
train_df = pd.read_csv('https://student-datasets-bucket.s3.ap-south-1.amazonaws.com/whitehat-ds-datasets/project-5/pulsar-star-prediction-train.csv')
test_df = pd.read_csv('https://student-datasets-bucket.s3.ap-south-1.amazonaws.com/whitehat-ds-datasets/project-5/pulsar-star-prediction-test.csv')

"""---

#### 2. Display First Five Rows

Display the first five rows of both the `train_df` and `test_df` DataFrames.
"""

# Display the first 5 rows of the 'train_df' DataFrame.
train_df.head()

# Display the first 5 rows of the 'test_df' DataFrame.
test_df.head()

"""---

#### 3. Display Last Five Rows

Display the last five rows of both the `train_df` and `test_df` DataFrames.
"""

# Display the last 5 rows of the 'train_df' DataFrame.
train_df.tail()

# Display the last 5 rows of the 'test_df' DataFrame.
test_df.tail()

"""---

#### 4. Rows & Columns In Train DataFrame

Find the number of rows and columns in both the `train_df` and `test_df` DataFrames.
"""

# Print the number of rows and columns in both the DataFrames.
print(train_df.shape)
print(test_df.shape)

"""---

#### 5. Check For Missing Values 

Check whether any of the columns in both the `train_df` and `test_df` DataFrames has any missing value.
"""

# Check for the missing values in the 'train_df' DataFrame.
num_missing_values = 0

for column in train_df.columns:
  for item in train_df[column].isnull():
    if item == True:
      num_missing_values += 1

num_missing_values

# Check for the missing values in the 'test_df' DataFrame.
num_missing_values = 0

for column in test_df.columns:
  for item in test_df[column].isnull():
    if item == True:
      num_missing_values += 1

num_missing_values

"""---

#### 6. Count The `0` & `1` Classes

Find out the number of `0` and `1` values in both the `train_df` and `test_df` DataFrames.
"""

# Print the count of the '0' and '1' classes in the 'train_df' DataFrame.
train_df['target_class'].value_counts()

# Print the count of the '0' and '1' classes in the 'test_df' DataFrame.
test_df['target_class'].value_counts()

"""---

#### 7. Feature Variables Extraction

Get the feature variables, i.e., `x_train` and `x_test` from both the `train_df` and `test_df` DataFrames respectively. Then, display the first 5 rows of `x_train` and `x_test` DataFrames.
"""

# Get the feature variables from the 'train_df' DataFrame.
x_train=train_df.iloc[:,1:]
# Display the first 5 rows of the 'x_train' DataFrame.
x_train.head()

# Get the feature variables from the 'test_df' DataFrame.
x_test=test_df.iloc[:,1:]
# Display the first 5 rows of the 'x_test' DataFrame.
x_test.head()

"""---

#### 8. Target Variable Extraction

Get the target variables, i.e., `y_train` and `y_test` from both the `train_df` and `test_df` DataFrames respectively. Then, display the first 5 rows of `y_train` and `y_test` Pandas series.
"""

# Get the target variable from the 'train_df' DataFrame.
y_train=train_df.iloc[:,0]
# Display the first 5 rows of the 'y_train' Pandas series.
y_train.head()

# Get the target variable from the 'test_df' DataFrame.
y_test=test_df.iloc[:,0]
# Display the first 5 rows of the 'y_test' Pandas series.
y_test.head()

"""---

#### 9. Building An XGBoost Classifier Model
"""

# Build A XGBoost Classifier model
# Import the xgboost module.
import xgboost as xg
# Import the confusion_matrix and classification_report modules.
from sklearn.metrics import classification_report,confusion_matrix

model=xg.XGBClassifier()
model.fit(x_train,y_train)
y_predicted=model.predict(x_test)
y_predicted

"""---

#### 10. Confusion Matrix & Classification Report

Print the confusion matrix and classification report to evaluate the model. Interpret and report the results obtained from the confusion matrix and the classification report.
"""

# Print the confusion matrix to see the number of TN, FN, TP and FP.
confusion_matrix(y_test,y_predicted)

# Print the precision, recall and f1-score values for both the '0' and '1' classes.
print(classification_report(y_test,y_predicted))

"""**Write your interpretation of the results here.**

- Interpretation 1: As the precision, recall and f-1 score values are closer to 1, it is good model

- Interpretation 2:xg boost model makes better predictions

- Interpretation 3:

  $\dots$

- Interpretation N:

---
"""